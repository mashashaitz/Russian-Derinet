{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the dictionary of kuznetsova into dict kuzwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleankuz(word):\n",
    "    w = word.replace('ё', 'е')\n",
    "    \n",
    "    if word == 'батрацкый':\n",
    "        w = 'батрацкий'\n",
    "    if word == 'грязиь':\n",
    "        w = 'грязь'\n",
    "    if word == 'кнутрй':\n",
    "        w = 'кнутри'\n",
    "    if word == 'трй':\n",
    "        w = 'три'\n",
    "    if word == 'молокй':\n",
    "        w = 'молокий'\n",
    "    if word == 'молочиый':\n",
    "        w = 'молочный'\n",
    "    if word == 'непостижиый':\n",
    "        w = 'непостижимый'\n",
    "    if word == 'окунеый':\n",
    "        w = 'окуневый'\n",
    "    if word == 'пригожй':\n",
    "        w = 'пригожий'\n",
    "    if word == 'уличиый':\n",
    "        w = 'уличный'\n",
    "    if word == 'минувшый':\n",
    "        w = 'минувший'\n",
    "    if word == 'сладенькнй':\n",
    "        w = 'сладенький'\n",
    "    if word == 'любый':\n",
    "        w = 'любой'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "kuzwords = {}\n",
    "doublers = {}\n",
    "with open('kuzefr_clean.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        if cleankuz(row[0]) in kuzwords:\n",
    "            doublers[cleankuz(row[0])] = kuzwords[cleankuz(row[0])][2]\n",
    "        r1 = row[1].replace('ё', 'е').replace(' ', '').replace('\"', '').replace('[', '').replace(']', '').replace(\"'\", '').split(',')\n",
    "        r2 = row[2].replace('ё', 'е').replace(' ', '').replace('\"', '').replace('[', '').replace(']', '').replace(\"'\", '').split(',')\n",
    "        kuzwords[cleankuz(row[0])] = [r1, r2, row[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72482"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kuzwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_kuzwords = deepcopy(kuzwords)\n",
    "kuzwords = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in old_kuzwords:\n",
    "    pos = old_kuzwords[w][2]\n",
    "    if pos in ['PRAEDIC', 'CONJ', 'PR', 'PARENTH', 'NONLEX']:\n",
    "        pos = m.analyze(w)[0]['analysis'][0]['gr'].split(',')[0].split('(')[0].split('=')[0]\n",
    "    if pos == 'A' or pos == 'ANUM' or pos == 'APRO':\n",
    "        kuzwords[w] = [old_kuzwords[w][0], old_kuzwords[w][1], 'A']\n",
    "    elif pos == 'V':\n",
    "        kuzwords[w] = [old_kuzwords[w][0], old_kuzwords[w][1], 'V']\n",
    "    elif pos == 'S' or pos == 'NUM' or pos == 'SPRO':\n",
    "        kuzwords[w] = [old_kuzwords[w][0], old_kuzwords[w][1], 'N']\n",
    "    elif pos == 'ADV' or pos == 'ADVPRO':\n",
    "        kuzwords[w] = [old_kuzwords[w][0], old_kuzwords[w][1], 'D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the dictionary of tixonov into dict tixwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantix(word):\n",
    "    w = word\n",
    "    w = w.split('/')[-1]\n",
    "    w = w.split('<')[0]\n",
    "    w = w.split('>')[-1]\n",
    "    \n",
    "    \n",
    "    w = w.replace('1', '')\n",
    "    w = w.replace('2', '')\n",
    "    w = w.replace('3', '')\n",
    "    w = w.replace('4', '')\n",
    "    w = w.replace('5', '')\n",
    "    w = w.replace('I', '')\n",
    "    w = w.replace('V', '')\n",
    "    w = w.replace('(', '')\n",
    "    w = w.replace(')', '')\n",
    "    \n",
    "    \n",
    "    if w.endswith(' '):\n",
    "        w = w = w[:-1]\n",
    "    if w.endswith('иjе'):\n",
    "        w = w[:-2] + 'е'\n",
    "    if w.endswith('иjа'):\n",
    "        w = w[:-2] + 'я'\n",
    "    if w.endswith('оjение'):\n",
    "        w = w[:-5] + 'ение'\n",
    "    if w.endswith('анjе'):\n",
    "        w = w[:-5] + 'анье'\n",
    "    if w.endswith('jецветный'):\n",
    "        w = w[:-9] + 'ецветный'\n",
    "    if w.endswith('jе'):\n",
    "        w = w[:-2] + 'ье'\n",
    "    if w.endswith('jо'):\n",
    "        w = w[:-2] + 'ьё'\n",
    "        \n",
    "        \n",
    "    if w.endswith('jа'):\n",
    "        w = w[:-2] + 'я'\n",
    "    if w.startswith('двоjе'):\n",
    "        w = w.replace('двоjе', 'двое')\n",
    "    if w.startswith('троj'):\n",
    "        w = w.replace('троj', 'тро')\n",
    "    if w.startswith('героjи'):\n",
    "        w = w.replace('героjи', 'герои')\n",
    "        \n",
    "        \n",
    "    w = w.replace('еjе', 'ее')\n",
    "    w = w.replace('сырjо', 'сырьё')\n",
    "    w = w.replace('строjитель', 'строитель')\n",
    "    w = w.replace('краjевой', 'краевой')\n",
    "    w = w.replace('настраjивать', 'настраивать')\n",
    "    w = w.replace('молниjе', 'молние')\n",
    "    w = w.replace('строjе', 'строе')\n",
    "    \n",
    "    \n",
    "    w = w.replace('любый', 'любой')\n",
    "    w = w.replace('хозя(ия)н)', 'хозяин')\n",
    "    w = w.replace('ничя', 'ничья')\n",
    "    \n",
    "    \n",
    "    if w == 'третjе':\n",
    "        w = 'третье'\n",
    "    if w == 'третjа':\n",
    "        w = 'третья'\n",
    "    if w == 'бjущийся':\n",
    "        w = 'бьющийся'\n",
    "    if w == 'боjеспособный':\n",
    "        w = 'боеспособный'\n",
    "    if w == 'обоjи':\n",
    "        w = 'обои'\n",
    "    if w == 'пробоjчик':\n",
    "        w = 'пробойчик'\n",
    "    if w == 'чаjеразвеска':\n",
    "        w = 'чаеразвеска'\n",
    "    if w == 'молниjевидный':\n",
    "        w = 'молниевидный'\n",
    "    if w == 'вjущий':\n",
    "        w = 'вьющий'\n",
    "    if w == 'своjевольный':\n",
    "        w = 'своевольный'\n",
    "    if w == 'голjом':\n",
    "        w = 'гольём'\n",
    "    if w == 'звенjевой':\n",
    "        w = 'звеньевой'\n",
    "    if w == 'гореванjице':\n",
    "        w = 'гореваньице'  \n",
    "    if w == 'кошачjи':\n",
    "        w = 'кошачьи'\n",
    "    if w == 'чешуjекрылый':\n",
    "        w = 'чешуекрылый'\n",
    "    if w == 'излиjание':\n",
    "        w = 'излияние'\n",
    "    if w == 'помальчишечjи':\n",
    "        w = 'по-мальчишечьи'\n",
    "    if w == 'промоjина':\n",
    "        w = 'промоина'\n",
    "    if w == 'радиjевый':\n",
    "        w = 'радиевый'\n",
    "    if w == 'поjущий':\n",
    "        w = 'поющий'\n",
    "    if w == 'пjущий':\n",
    "        w = 'пьющий'\n",
    "    if w == 'пjаный':\n",
    "        w = 'пьяный'\n",
    "    if w == 'паjивать':\n",
    "        w = 'паивать'\n",
    "    if w == 'запоjем':\n",
    "        w = 'запоем'\n",
    "    if w == 'своjекоштный':\n",
    "        w = 'своекоштный'\n",
    "    if w == 'семjой':\n",
    "        w = 'семьёй'   \n",
    "    if w == 'собачjи':\n",
    "        w = 'собачьи'\n",
    "    if w == 'стоjащий':\n",
    "        w = 'стоящий'\n",
    "    if w == 'стоjачий':\n",
    "        w = 'стоячий'\n",
    "    if w == 'стаjивать':\n",
    "        w = 'стаивать'\n",
    "    if w == 'твоjи':\n",
    "        w = 'твои'\n",
    "\n",
    "    \n",
    "    w = w.replace(' ', '').replace('ё', 'е')\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "tixwords = {}\n",
    "with open('nests.json', encoding='utf-8') as json_file:\n",
    "    nods = json.load(json_file)\n",
    "    for nod in nods:\n",
    "        for n in nod.values():\n",
    "            for word in n:\n",
    "                w = cleantix(word['root'])\n",
    "                if w not in tixwords:\n",
    "                    derivates = []\n",
    "                    for deriv in word['ambiguous']:\n",
    "                        d = cleantix(deriv)\n",
    "                        if d not in derivates and d != w:\n",
    "                            derivates.append(d)\n",
    "                    for deriv in word['non-ambiguous']:\n",
    "                        d = cleantix(deriv)\n",
    "                        if d not in derivates and d != w:\n",
    "                            derivates.append(d)\n",
    "                    tixwords[w] = derivates\n",
    "                else:\n",
    "                    derivates = tixwords[w]\n",
    "                    for deriv in word['ambiguous']:\n",
    "                        d = cleantix(deriv)\n",
    "                        if d not in derivates and d != w:\n",
    "                            derivates.append(d)\n",
    "                    for deriv in word['non-ambiguous']:\n",
    "                        d = cleantix(deriv)\n",
    "                        if d not in derivates and d != w:\n",
    "                            derivates.append(d)\n",
    "                    tixwords[w] = derivates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59563"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tixwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "tix_poses = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in tixwords:\n",
    "    if w not in kuzwords:\n",
    "        pos = m.analyze(w)[0]['analysis'][0]['gr'].split(',')[0].split('(')[0].split('=')[0]\n",
    "        if pos in ['S', 'A', 'D', 'V', 'ANUM', 'APRO', 'NUM', 'SPRO', 'ADV', 'ADVPRO']:\n",
    "            if pos == 'A' or pos == 'ANUM' or pos == 'APRO':\n",
    "                tix_poses[w] = 'A'\n",
    "            elif pos == 'V':\n",
    "                tix_poses[w] = 'V'\n",
    "            elif pos == 'S' or pos == 'NUM' or pos == 'SPRO':\n",
    "                tix_poses[w] = 'N'\n",
    "            elif pos == 'ADV' or pos == 'ADVPRO':\n",
    "                tix_poses[w] = 'D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25737"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tix_poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the pairs of lexemes into df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('df_pref.csv', encoding='utf-8')\n",
    "df_all = df_all[df_all.predictions_DecTree == 1] \n",
    "df_all = df_all.drop(columns=['Unnamed: 0', 'onset_p', 'onset_c', 'offset_p', 'offset_c', 'roots_p', 'roots_c', 'morphs_p', 'morphs_c', 'lev_dist', 'length', 'N_p', 'A_p', 'V_p', 'D_p', 'N_c', 'A_c', 'V_c', 'D_c', 'begins', 'ends', 'predictions_LogReg', 'predictions_DecTree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent</th>\n",
       "      <th>child</th>\n",
       "      <th>pos_p</th>\n",
       "      <th>pos_c</th>\n",
       "      <th>probs_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>август</td>\n",
       "      <td>августовский</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>авось</td>\n",
       "      <td>авоська</td>\n",
       "      <td>D</td>\n",
       "      <td>N</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>грузовой</td>\n",
       "      <td>автогрузовой</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>груз</td>\n",
       "      <td>автогрузовой</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>гуж</td>\n",
       "      <td>автогужевой</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        parent         child pos_p pos_c   probs_y\n",
       "0       август  августовский     N     A  1.000000\n",
       "71       авось       авоська     D     N  0.800000\n",
       "1110  грузовой  автогрузовой     A     A  0.571429\n",
       "1113      груз  автогрузовой     N     A  0.866667\n",
       "1264       гуж   автогужевой     N     A  0.866667"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single = pd.read_csv('singletons_rebound.csv', encoding='utf-8')\n",
    "df_single = df_single.drop(columns=['Unnamed: 0', 'predictions_LogReg', 'predictions_DecTree', 'predictions_LogReg1', 'predictions_DecTree1', 'final_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent</th>\n",
       "      <th>child</th>\n",
       "      <th>pos_p</th>\n",
       "      <th>pos_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>биография</td>\n",
       "      <td>автобиографический</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>автоген</td>\n",
       "      <td>автогенщик</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>автократический</td>\n",
       "      <td>автогенщик</td>\n",
       "      <td>A</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>автоген</td>\n",
       "      <td>автогенщик</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>автократический</td>\n",
       "      <td>автография</td>\n",
       "      <td>A</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            parent               child pos_p pos_c\n",
       "0        биография  автобиографический     N     A\n",
       "1          автоген          автогенщик     N     N\n",
       "2  автократический          автогенщик     A     N\n",
       "3          автоген          автогенщик     N     N\n",
       "4  автократический          автография     A     N"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_single.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unite data with singletons and use NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "nods = []\n",
    "for index, row in df_all.iterrows():\n",
    "    parent = row['parent'].replace('ё', 'е') + '#' + row['pos_p']\n",
    "    child = row['child'].replace('ё', 'е') + '#' + row['pos_c']\n",
    "    weight = row['probs_y']\n",
    "    if (child, parent, weight) not in nods:\n",
    "        nods.append((child, parent, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61389"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "FG = nx.Graph()\n",
    "FG.add_weighted_edges_from(nods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_nods = list(tree.maximum_spanning_edges(FG, algorithm='kruskal', data=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51820"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_nods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "NX_parents = []\n",
    "NX_children= []\n",
    "NX_pos_ps = []\n",
    "NX_pos_cs = []\n",
    "for nod in new_nods:\n",
    "    NX_parents.append(nod[0].split('#')[0])\n",
    "    NX_pos_ps.append(nod[0].split('#')[-1])\n",
    "    NX_children.append(nod[-1].split('#')[0])\n",
    "    NX_pos_cs.append(nod[-1].split('#')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame({'parent': NX_parents, 'child': NX_children, 'pos_p': NX_pos_ps, 'pos_c': NX_pos_cs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51820"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_all, df_single]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54647"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get rid of as many multiple parents as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the info on multiple parents into parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents = {}\n",
    "for index, row in df_all.iterrows():\n",
    "    parent = row['parent'].replace('ё', 'е')\n",
    "    child = row['child'].replace('ё', 'е')\n",
    "    if child not in parents:\n",
    "        parents[child] = [parent]\n",
    "    elif parent not in parents[child]:\n",
    "        parents[child].append(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44468"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get a list of bad parent - child combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_parents = []\n",
    "clean_parents = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_length = 0\n",
    "new_length = 1\n",
    "while new_length > old_length:\n",
    "    old_length = new_length\n",
    "    for child in parents:\n",
    "        if len(parents[child]) > 1:\n",
    "            same_roots = []\n",
    "            max_derivs = []\n",
    "            begins = []\n",
    "            ends = []\n",
    "\n",
    "            for parent in parents[child]:\n",
    "                # if possible, leave only the parents with the same rooot as the child\n",
    "                same_roots.append(len(set(kuzwords[parent][1]) & set(kuzwords[child][1])))\n",
    "\n",
    "\n",
    "                # if possible, leave the most derived parents to avoid multiple loops\n",
    "                max_deriv = 1\n",
    "                for other_parent in parents[child]:\n",
    "                    if other_parent != parent and other_parent in parents and not parent in parents:\n",
    "                        if parent in parents[other_parent]:\n",
    "                                max_deriv = 0\n",
    "                    elif other_parent != parent and other_parent in parents:\n",
    "                        if parent in parents[other_parent] and not other_parent in parents[parent]:\n",
    "                                max_deriv = 0\n",
    "                max_derivs.append(max_deriv)\n",
    "\n",
    "\n",
    "                # if possible prefer to have parents that are beginnings and endings of children\n",
    "                if child.startswith(parent):\n",
    "                    begins.append(1)\n",
    "                else:\n",
    "                    begins.append(0)\n",
    "                if child.endswith(parent):\n",
    "                    ends.append(1)\n",
    "                else:\n",
    "                    ends.append(0)\n",
    "\n",
    "\n",
    "            # get rid of outlandish roots\n",
    "            new_parents = copy(parents[child])\n",
    "            newer_parents = copy(parents[child])\n",
    "            if sum(same_roots) > 0 and sum(same_roots)/len(same_roots) < max(same_roots):\n",
    "                to_pop = []\n",
    "                for i, same in enumerate(same_roots):\n",
    "                    if same < max(same_roots):\n",
    "                        to_pop.append(i)\n",
    "                        if [new_parents[i], child] not in bad_parents:\n",
    "                            bad_parents.append([new_parents[i].replace('ё', 'е'), child.replace('ё', 'е')])\n",
    "                to_pop = to_pop[::-1]\n",
    "                for i in to_pop:\n",
    "                    same_roots.pop(i)\n",
    "                    newer_parents.pop(i)\n",
    "                    max_derivs.pop(i)\n",
    "                    begins.pop(i)\n",
    "                    ends.pop(i)\n",
    "            new_parents = copy(newer_parents)\n",
    "\n",
    "\n",
    "            # get rid of least derived\n",
    "            if 1 in max_derivs and 0 in max_derivs:\n",
    "                to_pop = []\n",
    "                for i, deriv in enumerate(max_derivs):\n",
    "                    if deriv == 0:\n",
    "                        if [new_parents[i], child] not in bad_parents:\n",
    "                            bad_parents.append([new_parents[i].replace('ё', 'е'), child.replace('ё', 'е')])\n",
    "                        to_pop.append(i)\n",
    "                to_pop = to_pop[::-1]\n",
    "                for i in to_pop:\n",
    "                    same_roots.pop(i)\n",
    "                    newer_parents.pop(i)\n",
    "                    max_derivs.pop(i)\n",
    "                    begins.pop(i)\n",
    "                    ends.pop(i)\n",
    "            new_parents = copy(newer_parents)\n",
    "\n",
    "\n",
    "            #get rid of not beginnings\n",
    "            if 1 in begins and 0 in begins:\n",
    "                to_pop = []\n",
    "                for i, begin in enumerate(begins):\n",
    "                    if begin == 0:\n",
    "                        if [new_parents[i], child] not in bad_parents:\n",
    "                            bad_parents.append([new_parents[i].replace('ё', 'е'), child.replace('ё', 'е')])\n",
    "                        to_pop.append(i)\n",
    "                to_pop = to_pop[::-1]\n",
    "                for i in to_pop:\n",
    "                    same_roots.pop(i)\n",
    "                    newer_parents.pop(i)\n",
    "                    max_derivs.pop(i)\n",
    "                    begins.pop(i)\n",
    "                    ends.pop(i)\n",
    "            new_parents = copy(newer_parents)\n",
    "\n",
    "\n",
    "            #get rid of not endings\n",
    "            if 1 in ends and 0 in ends:\n",
    "                to_pop = []\n",
    "                for i, end in enumerate(ends):\n",
    "                    if end == 0:\n",
    "                        if [new_parents[i], child] not in bad_parents:\n",
    "                            bad_parents.append([new_parents[i].replace('ё', 'е'), child.replace('ё', 'е')])\n",
    "                        to_pop.append(i)\n",
    "                to_pop = to_pop[::-1]\n",
    "                for i in to_pop:\n",
    "                    same_roots.pop(i)\n",
    "                    newer_parents.pop(i)\n",
    "                    max_derivs.pop(i)\n",
    "                    begins.pop(i)\n",
    "                    ends.pop(i)\n",
    "            new_parents = copy(newer_parents)\n",
    "\n",
    "\n",
    "            clean_parents[child] = new_parents\n",
    "    parents = clean_parents\n",
    "    new_length = len(bad_parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6654"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bad_parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_all.iterrows():\n",
    "    parent = row['parent'].replace('ё', 'е') + '#' + row['pos_p']\n",
    "    child = row['child'].replace('ё', 'е') + '#' + row['pos_c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create one dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unite all data on parts of speech into poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = {}\n",
    "for w in kuzwords:\n",
    "    if w not in poses:\n",
    "        poses[w] = [kuzwords[w][2]]\n",
    "    else:\n",
    "        poses[w].append(kuzwords[w][2])\n",
    "for w in tix_poses:\n",
    "    if w not in poses:\n",
    "        poses[w] = [tix_poses[w]]\n",
    "    else:\n",
    "        poses[w].append(tix_poses[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98100"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unite all data on parent-child pairs into children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59563"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tixwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = {}\n",
    "for w in tixwords:\n",
    "    if w not in children:\n",
    "        children[w] = []\n",
    "        for w2 in tixwords[w]:\n",
    "            if type(w2) == list:\n",
    "                for w3 in w2:\n",
    "                    if w3 not in children[w]:\n",
    "                        children[w].append(w3)\n",
    "            else:\n",
    "                if w2 not in children[w]:\n",
    "                    children[w].append(w2)\n",
    "    else:\n",
    "        for w2 in tixwords[w]:\n",
    "            if type(w2) == list:\n",
    "                for w3 in w2:\n",
    "                    if w3 not in children[w]:\n",
    "                        children[w].append(w3)\n",
    "            else:\n",
    "                if w2 not in children[w]:\n",
    "                    children[w].append(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59563"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_all.iterrows():\n",
    "    parent = row['parent'].replace('ё', 'е')\n",
    "    child = row['child'].replace('ё', 'е')\n",
    "    if [parent, child] not in bad_parents:\n",
    "        if parent not in children and parent != child:\n",
    "            children[parent] = [child]\n",
    "        elif parent != child:\n",
    "            children[parent].append([child])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75114"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_ON = []\n",
    "for c in all_children:\n",
    "    if type(c) != list:\n",
    "        if c not in children:\n",
    "            if c not in to_ON:\n",
    "                to_ON.append(c)\n",
    "    else:\n",
    "        for ch in c:\n",
    "            if not children[ch]:\n",
    "                if ch not in to_ON:\n",
    "                    to_ON.append(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find lexemes without parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_children = []\n",
    "for parent in children:\n",
    "    for child in children[parent]:\n",
    "        if type(child) != list:\n",
    "            if child not in all_children:\n",
    "                all_children.append(child)\n",
    "        else:\n",
    "            for c in child:\n",
    "                if c not in all_children:\n",
    "                    all_children.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8114"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_parents = []\n",
    "for parent in children:\n",
    "    if parent not in all_children:\n",
    "        no_parents.append(parent)\n",
    "len(no_parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90089"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_children + no_parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_masks_tixwords = {}\n",
    "with open('nests.json', encoding='utf-8') as json_file:\n",
    "    nods = json.load(json_file)\n",
    "    for nod in nods:\n",
    "        for n in nod.values():\n",
    "            for word in n:\n",
    "                if 'знать' == cleantix(word['root']) and 'II' in word['root']:\n",
    "                    w = 'знать#N'\n",
    "                elif 'знать' == cleantix(word['root']):\n",
    "                    w = 'знать#V'\n",
    "                elif 'честь' == cleantix(word['root']) and 'II' in word['root']:\n",
    "                    w = 'честь#V'\n",
    "                elif 'честь' == cleantix(word['root']):\n",
    "                    w = 'честь#N'\n",
    "                elif 'печь' == cleantix(word['root']) and 'II' in word['root']:\n",
    "                    w = 'печь#V'\n",
    "                elif 'печь' == cleantix(word['root']):\n",
    "                    w = 'печь#N'\n",
    "                elif 'мочь' == cleantix(word['root']) and 'II' in word['root']:\n",
    "                    w = 'мочь#N'\n",
    "                elif 'мочь' == cleantix(word['root']):\n",
    "                    w = 'мочь#V'\n",
    "                elif cleantix(word['root']) in poses:\n",
    "                    w = cleantix(word['root']) + '#' + poses[cleantix(word['root'])][0]\n",
    "                if w not in word_masks_tixwords and cleantix(word['root']) in poses:\n",
    "                    derivates = []\n",
    "                    for deriv in word['ambiguous']:\n",
    "                        if cleantix(deriv) in poses:\n",
    "                            d = cleantix(deriv) + '#' + poses[cleantix(deriv)][0]\n",
    "                            if d not in derivates and d != w:\n",
    "                                derivates.append(d)\n",
    "                    for deriv in word['non-ambiguous']:\n",
    "                        if cleantix(deriv) in poses:\n",
    "                            d = cleantix(deriv) + '#' + poses[cleantix(deriv)][0]\n",
    "                            if d not in derivates and d != w:\n",
    "                                derivates.append(d)\n",
    "                    word_masks_tixwords[w] = derivates\n",
    "                elif cleantix(word['root']) in poses:\n",
    "                    derivates = word_masks_tixwords[w]\n",
    "                    for deriv in word['ambiguous']:\n",
    "                        if cleantix(deriv) in poses:\n",
    "                            d = cleantix(deriv) + '#' + poses[cleantix(deriv)][0]\n",
    "                            if d not in derivates and d != w:\n",
    "                                derivates.append(d)\n",
    "                    for deriv in word['non-ambiguous']:\n",
    "                        if cleantix(deriv) in poses:\n",
    "                            d = cleantix(deriv) + '#' + poses[cleantix(deriv)][0]\n",
    "                            if d not in derivates and d != w:\n",
    "                                derivates.append(d)\n",
    "                    word_masks_tixwords[w] = derivates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59518"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_masks_tixwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tix_lemmas = []\n",
    "for w in tixwords:\n",
    "    if w not in all_tix_lemmas:\n",
    "        all_tix_lemmas.append(w)\n",
    "    for w2 in tixwords[w]:\n",
    "        if w2 not in all_tix_lemmas:\n",
    "            all_tix_lemmas.append(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_all.iterrows():\n",
    "    parent = row['parent'].replace('ё', 'е')\n",
    "    child = row['child'].replace('ё', 'е')\n",
    "    if parent not in tixwords or child not in tixwords:\n",
    "        parent = parent + '#' + poses[parent][0]\n",
    "        child = child + '#' + poses[child][0]\n",
    "        if [parent, child] not in bad_parents:\n",
    "            if parent not in word_masks_tixwords and parent != child:\n",
    "                word_masks_tixwords[parent] = [child]\n",
    "            elif parent != child:\n",
    "                word_masks_tixwords[parent].append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76993"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_masks_tixwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10067"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_parents = []\n",
    "word_masks_children = []\n",
    "for parent in word_masks_tixwords:\n",
    "    word_masks_children.extend(word_masks_tixwords[parent])\n",
    "for parent in word_masks_tixwords:\n",
    "    if parent not in word_masks_children:\n",
    "        no_parents.append(parent)\n",
    "len(no_parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get words with two roots into two_parents dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_parents = {}\n",
    "for parent in word_masks_tixwords:\n",
    "    for child in word_masks_tixwords[parent]:\n",
    "        new_child = child\n",
    "        if type(child) == list:\n",
    "            new_child = new_child[0]\n",
    "            \n",
    "            \n",
    "        if new_child not in morph_parents:\n",
    "            morph_parents[new_child] = [parent]\n",
    "        else:\n",
    "            morph_parents[new_child].append(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "global two_parents \n",
    "two_parents = {}\n",
    "for child in morph_parents:\n",
    "    new_child = child\n",
    "    if type(child) == list:\n",
    "            new_child = new_child[0]\n",
    "    raw_child = child.split('#')[0]\n",
    "            \n",
    "            \n",
    "    if len(morph_parents[new_child]) == 2:\n",
    "        parent1 = morph_parents[new_child][0].split('#')[0]\n",
    "        parent2 = morph_parents[new_child][1].split('#')[0]\n",
    "        if parent1 in kuzwords and parent2 in kuzwords:\n",
    "            if len(set(kuzwords[parent1][1]) & set(kuzwords[parent2][1])) == 0:\n",
    "                two_parents[new_child] = [morph_parents[new_child][0], morph_parents[new_child][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find and eradicate loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_mask in word_masks_tixwords:\n",
    "    for word_mask2 in word_masks_tixwords[word_mask]:\n",
    "        if word_mask2 in word_masks_tixwords:\n",
    "            if word_mask in word_masks_tixwords[word_mask2]:\n",
    "                if len(word_mask2) > len(word_mask):\n",
    "                    word_masks_tixwords[word_mask2].remove(word_mask)\n",
    "                else:\n",
    "                    word_masks_tixwords[word_mask].remove(word_mask2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get to derinet format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the format is as following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 id 2 word#N 3 word 4 NOUN / ADJ / VERB / ADV 5 empty 6 empty 7 id 8 Type=Derivation Type=Com 9 empty 10 morphs + second parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but first we've got to clean things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_masks_tixwords['ворон#N'] = ['вороненый#A', 'вороненький#A', 'воронить#V','ворониха#N', 'вороновые#A']\n",
    "word_masks_tixwords['верша#N'] = []\n",
    "word_masks_tixwords['плат#N'] = ['платать#V', 'плахта#N']\n",
    "word_masks_tixwords['платать#V'] = []\n",
    "word_masks_tixwords['колок#N'] = ['коловорот#N', 'коловращение#N']\n",
    "word_masks_tixwords['колос#N'] = ['колосовые#A', 'колосистый#A', 'колосник#N', 'колосовой#A', 'пустоколосый#A', 'колосок#N', 'колоситься#V']\n",
    "word_masks_tixwords['общерусский#A'] = []\n",
    "word_masks_tixwords['тяжеловес#N'] = ['полутяжеловес#N']\n",
    "word_masks_tixwords['полутяжеловес#N'] = []\n",
    "word_masks_tixwords['наводить#V'] = ['наводиться#V', 'наводка#N', 'наводчик#N', 'наводящий#A', 'наводной#A', 'наводка#N','наводчик#N']\n",
    "word_masks_tixwords['веретенообразный#A'] = []\n",
    "word_masks_tixwords['стеклообразный#A'] = []\n",
    "word_masks_tixwords['дровозаготовка#N'] = []\n",
    "word_masks_tixwords['западнорусский#A'] = []\n",
    "word_masks_tixwords['шерстезаготовительный#A'] = []\n",
    "word_masks_tixwords['боеподготовка#N'] = []\n",
    "word_masks_tixwords['артподготовка#N'] = []\n",
    "word_masks_tixwords['боеготовность#N'] = []\n",
    "word_masks_tixwords['нефтепромыслы#N'] = []\n",
    "word_masks_tixwords['нефтеаппаратура#N'] = []\n",
    "word_masks_tixwords['белградский#A'] = []\n",
    "word_masks_tixwords['град#N'] = []\n",
    "word_masks_tixwords['свиль#N'] = []\n",
    "word_masks_tixwords['взвивать#V'] = ['взвиваться#V', 'взвивание#N', 'взвить#V']\n",
    "word_masks_tixwords['витый#V'] = []\n",
    "word_masks_tixwords['водораспределитель#N'] = ['водораспределительный#A']\n",
    "word_masks_tixwords['менять#V'].append('вменять#V')\n",
    "word_masks_tixwords['вменять#V'] = []\n",
    "word_masks_tixwords['водораспределительный#A'] = []\n",
    "word_masks_tixwords['воздухораспределитель#N'] = []\n",
    "word_masks_tixwords['великолукский#A'] = []\n",
    "word_masks_tixwords['жилищностроительный#A'] = []\n",
    "word_masks_tixwords['навесистый#A'] = []\n",
    "\n",
    "\n",
    "if 'лучистый#A' in word_masks_tixwords['лук#N']:\n",
    "    word_masks_tixwords['лук#N'].remove('лучистый#A')\n",
    "    \n",
    "    \n",
    "if 'кольцо#N' in word_masks_tixwords['колоть#V']:\n",
    "    word_masks_tixwords['колоть#V'].remove('кольцо#N')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare some data for future statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "global current_max_width\n",
    "current_max_width = 0\n",
    "\n",
    "global trees_max_width\n",
    "trees_max_width = []\n",
    "\n",
    "global trees_max_depth\n",
    "trees_max_depth = []\n",
    "\n",
    "global current_max_depth\n",
    "current_max_depth = 0\n",
    "\n",
    "global current_depth\n",
    "current_depth = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare another filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_bad_verb_parent(parent, child, kuzwords):\n",
    "    true = True\n",
    "    \n",
    "    \n",
    "    suffixes = ['вы','пере','про', 'при', 'у', 'за','из', 'на', 'по', 'под', 'с', 'о', 'рас', 'раз', 'в', 'до']\n",
    "    if parent.endswith('ть') or parent.endswith('ться'):\n",
    "        for s in suffixes:\n",
    "            if parent.startswith(s):\n",
    "                if child in kuzwords and parent in kuzwords:\n",
    "                    if not set(kuzwords[parent][1]) & set(kuzwords[child][1]):\n",
    "                        true = False\n",
    "    if child.endswith('ть') or child.endswith('ться'):\n",
    "        for s in suffixes:\n",
    "            if child.startswith(s):\n",
    "                if child in kuzwords and parent in kuzwords:\n",
    "                    if not set(kuzwords[parent][1]) & set(kuzwords[child][1]):\n",
    "                        true = False\n",
    "                        \n",
    "                        \n",
    "    if parent == 'полу' + child:\n",
    "        true = False\n",
    "        \n",
    "    \n",
    "    if parent == 'не' + child:\n",
    "        true = False\n",
    "        \n",
    "        \n",
    "    if parent == 'само' + child:\n",
    "        true = False\n",
    "        \n",
    "        \n",
    "    if parent == child + 'ся':\n",
    "        true = False\n",
    "        \n",
    "        \n",
    "    if parent.endswith('ведение') and child.endswith('ведение'):\n",
    "        if parent in kuzwords and child in kuzwords:\n",
    "            if len(set(kuzwords[parent][1]) & set(kuzwords[child][1])) < 2:\n",
    "                true = False\n",
    "        else:\n",
    "            true = False\n",
    "            \n",
    "    \n",
    "    if parent.endswith('вед') and child.endswith('вед'):\n",
    "        if len(set(kuzwords[parent][1]) & set(kuzwords[child][1])) < 2:\n",
    "            true = False\n",
    "            \n",
    "    \n",
    "    if parent.endswith('о' + child) and len(kuzwords[parent][1]) > 1:\n",
    "        true = False\n",
    "        \n",
    "        \n",
    "    if parent.endswith('е' + child) and len(kuzwords[parent][1]) > 1:\n",
    "        true = False\n",
    "        \n",
    "        \n",
    "    if parent + 'ца' == child:\n",
    "        true = False\n",
    "    \n",
    "        \n",
    "    other = ['не','само']\n",
    "    for o in other:\n",
    "        if parent.startswith(o) or child.startswith(o):\n",
    "            if parent in kuzwords and child in kuzwords:\n",
    "                if not set(kuzwords[parent][1]) & set(kuzwords[child][1]):\n",
    "                    true = False\n",
    "            else:\n",
    "                if parent not in child and child not in parent:\n",
    "                    true = False\n",
    "    return true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the info needed for the UDer format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_derinet(t, parent_id, parent, child_tag):\n",
    "    global current_max_width\n",
    "    global trees_max_width\n",
    "    global trees_max_depth\n",
    "    global current_max_depth\n",
    "    global current_depth\n",
    "    \n",
    "    if type(child_tag) == list:\n",
    "        child_tag = child_tag[0]\n",
    "        \n",
    "    \n",
    "    if t > current_max_width:\n",
    "        current_max_width = t\n",
    "    if t > trees_max_width[-1]:\n",
    "        trees_max_width[-1] = t\n",
    "        \n",
    "\n",
    "    current_depth += 1\n",
    "    if current_depth > current_max_depth:\n",
    "        current_max_depth = current_depth\n",
    "    if trees_max_depth[-1] < current_max_depth:\n",
    "        trees_max_depth[-1] = current_max_depth\n",
    "\n",
    "        \n",
    "    child_id = parent_id.split('.')[0] + '.' + str(t) \n",
    "    t += 1\n",
    "    ids.append(child_id)\n",
    "    \n",
    "    \n",
    "    mask = child_tag\n",
    "    masks.append(child_tag)   \n",
    "    \n",
    "    \n",
    "    word = child_tag.split('#')[0]\n",
    "    words.append(word)\n",
    "    \n",
    "    \n",
    "    pos = child_tag.split('#')[-1]\n",
    "    if pos == 'N':\n",
    "        full_pos = 'NOUN'\n",
    "    elif pos == 'A':\n",
    "        full_pos = 'ADJ'\n",
    "    elif pos == 'V':\n",
    "        full_pos = 'VERB'\n",
    "    elif pos == 'D':\n",
    "        full_pos = 'ADV'\n",
    "    full_poses.append(full_pos)\n",
    "    \n",
    "    \n",
    "    if word in kuzwords:\n",
    "        morph = 'morphs: ' + ', '.join(kuzwords[word][0]) + '; roots: '+ ', '.join(kuzwords[word][1])\n",
    "    else:\n",
    "        morph = ''\n",
    "    morphs.append(morph)\n",
    "    \n",
    "    \n",
    "    par_id = parent_id\n",
    "    parent_ids.append(par_id)\n",
    "    \n",
    "    \n",
    "    if mask in two_parents:\n",
    "        derivation = 'Type=Com'\n",
    "        second_parent = 'parents: ' + ', '.join(two_parents[mask])\n",
    "    else:\n",
    "        derivation = 'Type=Derivation'\n",
    "        second_parent = ''\n",
    "    derivations.append(derivation)\n",
    "    second_parents.append(second_parent)\n",
    "    \n",
    "    \n",
    "    if child_tag in word_masks_tixwords and child_tag not in used:\n",
    "        for child in sorted(word_masks_tixwords[child_tag]):\n",
    "            used.append(child_tag)\n",
    "            if type(child) == list:\n",
    "                new_child = child[0]\n",
    "            else:\n",
    "                new_child = child\n",
    "            if is_not_bad_verb_parent(word, new_child.split('#')[0], kuzwords):\n",
    "                t = to_derinet(t, child_id, word, new_child)\n",
    "                current_depth -= 1\n",
    "    \n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global ids\n",
    "ids = []\n",
    "global masks\n",
    "masks = []\n",
    "global words\n",
    "words = []\n",
    "global full_poses\n",
    "full_poses = []\n",
    "global morphs\n",
    "morphs = []\n",
    "global parent_ids\n",
    "parent_ids = []\n",
    "global derivations\n",
    "derivations = []\n",
    "global second_parents\n",
    "second_parents = []\n",
    "global used\n",
    "used = []\n",
    "global t\n",
    "t = 1\n",
    "\n",
    "\n",
    "for i, parent in enumerate(sorted(no_parents)):\n",
    "    \n",
    "    \n",
    "    child_id = str(i+1) + '.0'\n",
    "    t = 1\n",
    "    ids.append(child_id)\n",
    "    \n",
    "    \n",
    "    mask = parent\n",
    "    masks.append(mask)\n",
    "    \n",
    "    \n",
    "    word = parent.split('#')[0]\n",
    "    words.append(word)\n",
    "    \n",
    "    \n",
    "    pos = parent.split('#')[-1]\n",
    "    if pos == 'N':\n",
    "        full_pos = 'NOUN'\n",
    "    elif pos == 'A':\n",
    "        full_pos = 'ADJ'\n",
    "    elif pos == 'V':\n",
    "        full_pos = 'VERB'\n",
    "    elif pos == 'D':\n",
    "        full_pos = 'ADV'\n",
    "    full_poses.append(full_pos)\n",
    "    \n",
    "    \n",
    "    if word in kuzwords:\n",
    "        morph = 'morphs: ' + ', '.join(kuzwords[word][0]) + '; roots: '+ ', '.join(kuzwords[word][1])\n",
    "    else:\n",
    "        morph = ''\n",
    "    morphs.append(morph)\n",
    "    \n",
    "    \n",
    "    parent_id = ''\n",
    "    parent_ids.append(parent_id)\n",
    "    \n",
    "    \n",
    "    derivation = ''\n",
    "    derivations.append(derivation)\n",
    "    \n",
    "    \n",
    "    second_parent = ''\n",
    "    second_parents.append(second_parent)\n",
    "    \n",
    "    if parent in word_masks_tixwords:\n",
    "        for child in sorted(word_masks_tixwords[parent]):\n",
    "            current_depth = 0\n",
    "            current_max_depth = 0\n",
    "            trees_max_depth.append(0)\n",
    "            current_max_width = 0\n",
    "            trees_max_width.append(0)\n",
    "            \n",
    "            if type(child) == list:\n",
    "                new_child = child[0]\n",
    "            else:\n",
    "                new_child = child\n",
    "            \n",
    "            if is_not_bad_verb_parent(word, new_child.split('#')[0], kuzwords):\n",
    "                t = to_derinet(t, child_id, word, new_child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "623"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(trees_max_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(trees_max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.5422753834916"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(trees_max_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3748782566350133"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(trees_max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88180"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have files made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "with open('ru-der-NX.tsv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "    \n",
    "    \n",
    "    for i in range(len(ids)):\n",
    "        child_id = ids[i]\n",
    "        mask = masks[i]\n",
    "        word = words[i]\n",
    "        full_pos = full_poses[i]\n",
    "        empty = ''\n",
    "        morph = morphs[i]\n",
    "        parent_id = parent_ids[i]\n",
    "        derivation = derivations[i]\n",
    "        second_parent = second_parents[i]\n",
    "        \n",
    "        \n",
    "        if int(child_id.split('.')[0]) != t:\n",
    "            t = int(child_id.split('.')[0])\n",
    "            csvwriter.writerow(['', '', '', '', '', '', '', '', '', ''])\n",
    "            \n",
    "        csvwriter.writerow([child_id, mask, word, full_pos, empty, empty, parent_id, derivation, empty, ' '.join([morph, second_parent])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "duplicates = []\n",
    "with open('ru-der-no-duplicates-NX.tsv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "    \n",
    "    for i in range(len(ids)):\n",
    "        child_id = ids[i]\n",
    "        mask = masks[i]\n",
    "        word = words[i]\n",
    "        full_pos = full_poses[i]\n",
    "        empty = ''\n",
    "        morph = morphs[i]\n",
    "        parent_id = parent_ids[i]\n",
    "        derivation = derivations[i]\n",
    "        second_parent = second_parents[i]\n",
    "        \n",
    "        \n",
    "        if mask not in duplicates:\n",
    "            duplicates.append(mask)\n",
    "            \n",
    "            \n",
    "            if int(child_id.split('.')[0]) != t:\n",
    "                t = int(child_id.split('.')[0])\n",
    "                csvwriter.writerow(['', '', '', '', '', '', '', '', '', ''])\n",
    "\n",
    "\n",
    "            csvwriter.writerow([child_id, mask, word, full_pos, empty, empty, parent_id, derivation, empty, ' '.join([morph, second_parent])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents = {}\n",
    "for mask in word_masks_tixwords:\n",
    "    parent = mask.split('#')[0]\n",
    "    for mask2 in word_masks_tixwords[mask]:\n",
    "        child = mask2.split('#')[0]\n",
    "        if child not in parents and is_not_bad_verb_parent(parent, child, kuzwords):\n",
    "            parents[child] = [parent]\n",
    "        elif child in parents and is_not_bad_verb_parent(parent, child, kuzwords):\n",
    "            if parent not in parents[child]:\n",
    "                parents[child].append(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parent_check.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "    i = 0\n",
    "    for child in random.sample(parents.keys(), 2500):\n",
    "        if i < 100 and child not in tixwords:\n",
    "            i += 1\n",
    "            row = (child + ' ' + ' '.join(parents[child])).split(' ')\n",
    "            csvwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('no_parent_check.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "    i = 0\n",
    "    for child in random.sample(no_parents, 250):\n",
    "        if i < 100 and child.split('#')[0] not in tixwords:\n",
    "            i += 1\n",
    "            csvwriter.writerow([child.split('#')[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

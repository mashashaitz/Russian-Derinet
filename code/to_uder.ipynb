{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the dictionary of kuznetsova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleankuz(word):\n",
    "    w = word.replace('ё', 'е')\n",
    "    \n",
    "    if word == 'батрацкый':\n",
    "        w = 'батрацкий'\n",
    "    if word == 'грязиь':\n",
    "        w = 'грязь'\n",
    "    if word == 'кнутрй':\n",
    "        w = 'кнутри'\n",
    "    if word == 'трй':\n",
    "        w = 'три'\n",
    "    if word == 'молокй':\n",
    "        w = 'молокий'\n",
    "    if word == 'молочиый':\n",
    "        w = 'молочный'\n",
    "    if word == 'непостижиый':\n",
    "        w = 'непостижимый'\n",
    "    if word == 'окунеый':\n",
    "        w = 'окуневый'\n",
    "    if word == 'пригожй':\n",
    "        w = 'пригожий'\n",
    "    if word == 'уличиый':\n",
    "        w = 'уличный'\n",
    "    if word == 'минувшый':\n",
    "        w = 'минувший'\n",
    "    if word == 'сладенькнй':\n",
    "        w = 'сладенький'\n",
    "    if word == 'любый':\n",
    "        w = 'любой'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "kuzwords = {}\n",
    "doublers = {}\n",
    "with open('kuzdra_clean.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        if cleankuz(row[0]) in kuzwords:\n",
    "            doublers[cleankuz(row[0])] = kuzwords[cleankuz(row[0])][2]\n",
    "        r1 = row[1].replace('ё', 'е').replace(' ', '').replace('\"', '').replace('[', '').replace(']', '').replace(\"'\", '').split(',')\n",
    "        r2 = row[2].replace('ё', 'е').replace(' ', '').replace('\"', '').replace('[', '').replace(']', '').replace(\"'\", '').split(',')\n",
    "        kuzwords[cleankuz(row[0])] = [r1, r2, row[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72482"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kuzwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_kuzwords = deepcopy(kuzwords)\n",
    "kuzwords = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in old_kuzwords:\n",
    "    pos = old_kuzwords[w][2]\n",
    "    if pos in ['PRAEDIC', 'CONJ', 'PR', 'PARENTH', 'NONLEX']:\n",
    "        pos = m.analyze(w)[0]['analysis'][0]['gr'].split(',')[0].split('(')[0].split('=')[0]\n",
    "    if pos == 'A' or pos == 'ANUM' or pos == 'APRO':\n",
    "        kuzwords[w] = [old_kuzwords[w][0], old_kuzwords[w][1], 'A']\n",
    "    elif pos == 'V':\n",
    "        kuzwords[w] = [old_kuzwords[w][0], old_kuzwords[w][1], 'V']\n",
    "    elif pos == 'S' or pos == 'NUM' or pos == 'SPRO':\n",
    "        kuzwords[w] = [old_kuzwords[w][0], old_kuzwords[w][1], 'N']\n",
    "    elif pos == 'ADV' or pos == 'ADVPRO':\n",
    "        kuzwords[w] = [old_kuzwords[w][0], old_kuzwords[w][1], 'D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the dictionary of tixonov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantix(word):\n",
    "    w = word\n",
    "    w = w.split('/')[-1]\n",
    "    w = w.split('<')[0]\n",
    "    w = w.split('>')[-1]\n",
    "    \n",
    "    \n",
    "    w = w.replace('1', '')\n",
    "    w = w.replace('2', '')\n",
    "    w = w.replace('3', '')\n",
    "    w = w.replace('4', '')\n",
    "    w = w.replace('5', '')\n",
    "    w = w.replace('I', '')\n",
    "    w = w.replace('V', '')\n",
    "    w = w.replace('(', '')\n",
    "    w = w.replace(')', '')\n",
    "    \n",
    "    \n",
    "    if w.endswith(' '):\n",
    "        w = w = w[:-1]\n",
    "    if w.endswith('иjе'):\n",
    "        w = w[:-2] + 'е'\n",
    "    if w.endswith('иjа'):\n",
    "        w = w[:-2] + 'я'\n",
    "    if w.endswith('оjение'):\n",
    "        w = w[:-5] + 'ение'\n",
    "    if w.endswith('анjе'):\n",
    "        w = w[:-5] + 'анье'\n",
    "    if w.endswith('jецветный'):\n",
    "        w = w[:-9] + 'ецветный'\n",
    "    if w.endswith('jе'):\n",
    "        w = w[:-2] + 'ье'\n",
    "    if w.endswith('jо'):\n",
    "        w = w[:-2] + 'ьё'\n",
    "        \n",
    "        \n",
    "    if w.endswith('jа'):\n",
    "        w = w[:-2] + 'я'\n",
    "    if w.startswith('двоjе'):\n",
    "        w = w.replace('двоjе', 'двое')\n",
    "    if w.startswith('троj'):\n",
    "        w = w.replace('троj', 'тро')\n",
    "    if w.startswith('героjи'):\n",
    "        w = w.replace('героjи', 'герои')\n",
    "        \n",
    "        \n",
    "    w = w.replace('еjе', 'ее')\n",
    "    w = w.replace('сырjо', 'сырьё')\n",
    "    w = w.replace('строjитель', 'строитель')\n",
    "    w = w.replace('краjевой', 'краевой')\n",
    "    w = w.replace('настраjивать', 'настраивать')\n",
    "    w = w.replace('молниjе', 'молние')\n",
    "    w = w.replace('строjе', 'строе')\n",
    "    \n",
    "    \n",
    "    w = w.replace('любый', 'любой')\n",
    "    w = w.replace('хозя(ия)н)', 'хозяин')\n",
    "    w = w.replace('ничя', 'ничья')\n",
    "    \n",
    "    \n",
    "    if w == 'третjе':\n",
    "        w = 'третье'\n",
    "    if w == 'третjа':\n",
    "        w = 'третья'\n",
    "    if w == 'бjущийся':\n",
    "        w = 'бьющийся'\n",
    "    if w == 'боjеспособный':\n",
    "        w = 'боеспособный'\n",
    "    if w == 'обоjи':\n",
    "        w = 'обои'\n",
    "    if w == 'пробоjчик':\n",
    "        w = 'пробойчик'\n",
    "    if w == 'чаjеразвеска':\n",
    "        w = 'чаеразвеска'\n",
    "    if w == 'молниjевидный':\n",
    "        w = 'молниевидный'\n",
    "    if w == 'вjущий':\n",
    "        w = 'вьющий'\n",
    "    if w == 'своjевольный':\n",
    "        w = 'своевольный'\n",
    "    if w == 'голjом':\n",
    "        w = 'гольём'\n",
    "    if w == 'звенjевой':\n",
    "        w = 'звеньевой'\n",
    "    if w == 'гореванjице':\n",
    "        w = 'гореваньице'  \n",
    "    if w == 'кошачjи':\n",
    "        w = 'кошачьи'\n",
    "    if w == 'чешуjекрылый':\n",
    "        w = 'чешуекрылый'\n",
    "    if w == 'излиjание':\n",
    "        w = 'излияние'\n",
    "    if w == 'помальчишечjи':\n",
    "        w = 'по-мальчишечьи'\n",
    "    if w == 'промоjина':\n",
    "        w = 'промоина'\n",
    "    if w == 'радиjевый':\n",
    "        w = 'радиевый'\n",
    "    if w == 'поjущий':\n",
    "        w = 'поющий'\n",
    "    if w == 'пjущий':\n",
    "        w = 'пьющий'\n",
    "    if w == 'пjаный':\n",
    "        w = 'пьяный'\n",
    "    if w == 'паjивать':\n",
    "        w = 'паивать'\n",
    "    if w == 'запоjем':\n",
    "        w = 'запоем'\n",
    "    if w == 'своjекоштный':\n",
    "        w = 'своекоштный'\n",
    "    if w == 'семjой':\n",
    "        w = 'семьёй'   \n",
    "    if w == 'собачjи':\n",
    "        w = 'собачьи'\n",
    "    if w == 'стоjащий':\n",
    "        w = 'стоящий'\n",
    "    if w == 'стоjачий':\n",
    "        w = 'стоячий'\n",
    "    if w == 'стаjивать':\n",
    "        w = 'стаивать'\n",
    "    if w == 'твоjи':\n",
    "        w = 'твои'\n",
    "\n",
    "    \n",
    "    w = w.replace(' ', '').replace('ё', 'е')\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "tixwords = {}\n",
    "with open('nests.json', encoding='utf-8') as json_file:\n",
    "    nods = json.load(json_file)\n",
    "    for nod in nods:\n",
    "        for n in nod.values():\n",
    "            for word in n:\n",
    "                w = cleantix(word['root'])\n",
    "                if w not in tixwords:\n",
    "                    derivates = []\n",
    "                    for deriv in word['ambiguous']:\n",
    "                        d = cleantix(deriv)\n",
    "                        if d not in derivates and d != w:\n",
    "                            derivates.append(d)\n",
    "                    for deriv in word['non-ambiguous']:\n",
    "                        d = cleantix(deriv)\n",
    "                        if d not in derivates and d != w:\n",
    "                            derivates.append(d)\n",
    "                    tixwords[w] = derivates\n",
    "                else:\n",
    "                    derivates = tixwords[w]\n",
    "                    for deriv in word['ambiguous']:\n",
    "                        d = cleantix(deriv)\n",
    "                        if d not in derivates and d != w:\n",
    "                            derivates.append(d)\n",
    "                    for deriv in word['non-ambiguous']:\n",
    "                        d = cleantix(deriv)\n",
    "                        if d not in derivates and d != w:\n",
    "                            derivates.append(d)\n",
    "                    tixwords[w] = derivates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59563"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tixwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "tix_poses = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in tixwords:\n",
    "    if w not in kuzwords:\n",
    "        pos = m.analyze(w)[0]['analysis'][0]['gr'].split(',')[0].split('(')[0].split('=')[0]\n",
    "        if pos in ['S', 'A', 'D', 'V', 'ANUM', 'APRO', 'NUM', 'SPRO', 'ADV', 'ADVPRO']:\n",
    "            if pos == 'A' or pos == 'ANUM' or pos == 'APRO':\n",
    "                tix_poses[w] = 'A'\n",
    "            elif pos == 'V':\n",
    "                tix_poses[w] = 'V'\n",
    "            elif pos == 'S' or pos == 'NUM' or pos == 'SPRO':\n",
    "                tix_poses[w] = 'N'\n",
    "            elif pos == 'ADV' or pos == 'ADVPRO':\n",
    "                tix_poses[w] = 'D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25737"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tix_poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('final_pairs.csv', encoding='utf-8')\n",
    "df_all = df_all.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent</th>\n",
       "      <th>child</th>\n",
       "      <th>pos_p</th>\n",
       "      <th>pos_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>авось</td>\n",
       "      <td>авоська</td>\n",
       "      <td>D</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>автократия</td>\n",
       "      <td>автократический</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>автократ</td>\n",
       "      <td>автократия</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>автотипия</td>\n",
       "      <td>автотипический</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>автохром</td>\n",
       "      <td>автохромный</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       parent            child pos_p pos_c\n",
       "0       авось          авоська     D     N\n",
       "1  автократия  автократический     N     A\n",
       "2    автократ       автократия     N     N\n",
       "3   автотипия   автотипический     N     A\n",
       "4    автохром      автохромный     N     A"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get rid of as many multiple parents as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get info on multiple parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents = {}\n",
    "for index, row in df_all.iterrows():\n",
    "    parent = row['parent'].replace('ё', 'е')\n",
    "    child = row['child'].replace('ё', 'е')\n",
    "    if child not in parents:\n",
    "        parents[child] = [parent]\n",
    "    elif parent not in parents[child]:\n",
    "        parents[child].append(parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get a list of bad parenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_parents = []\n",
    "clean_parents = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_length = 0\n",
    "new_length = 1\n",
    "while new_length > old_length:\n",
    "    old_length = new_length\n",
    "    for child in parents:\n",
    "        if len(parents[child]) > 1:\n",
    "            same_roots = []\n",
    "            max_derivs = []\n",
    "            begins = []\n",
    "            ends = []\n",
    "\n",
    "            for parent in parents[child]:\n",
    "                # if possible, leave only the parents with the same rooot as the child\n",
    "                same_roots.append(len(set(kuzwords[parent][1]) & set(kuzwords[child][1])))\n",
    "\n",
    "\n",
    "                # if possible, leave the most derived parents to avoid multiple loops\n",
    "                max_deriv = 1\n",
    "                for other_parent in parents[child]:\n",
    "                    if other_parent != parent and other_parent in parents and not parent in parents:\n",
    "                        if parent in parents[other_parent]:\n",
    "                                max_deriv = 0\n",
    "                    elif other_parent != parent and other_parent in parents:\n",
    "                        if parent in parents[other_parent] and not other_parent in parents[parent]:\n",
    "                                max_deriv = 0\n",
    "                max_derivs.append(max_deriv)\n",
    "\n",
    "\n",
    "                # if possible prefer to have parents that are beginnings and endings of children\n",
    "                if child.startswith(parent):\n",
    "                    begins.append(1)\n",
    "                else:\n",
    "                    begins.append(0)\n",
    "                if child.endswith(parent):\n",
    "                    ends.append(1)\n",
    "                else:\n",
    "                    ends.append(0)\n",
    "\n",
    "\n",
    "            # get rid of outlandish roots\n",
    "            new_parents = copy(parents[child])\n",
    "            newer_parents = copy(parents[child])\n",
    "            if sum(same_roots) > 0 and sum(same_roots)/len(same_roots) < max(same_roots):\n",
    "                to_pop = []\n",
    "                for i, same in enumerate(same_roots):\n",
    "                    if same < max(same_roots):\n",
    "                        to_pop.append(i)\n",
    "                        if [new_parents[i], child] not in bad_parents:\n",
    "                            bad_parents.append([new_parents[i].replace('ё', 'е'), child.replace('ё', 'е')])\n",
    "                to_pop = to_pop[::-1]\n",
    "                for i in to_pop:\n",
    "                    same_roots.pop(i)\n",
    "                    newer_parents.pop(i)\n",
    "                    max_derivs.pop(i)\n",
    "                    begins.pop(i)\n",
    "                    ends.pop(i)\n",
    "            new_parents = copy(newer_parents)\n",
    "\n",
    "\n",
    "            # get rid of least derived\n",
    "            if 1 in max_derivs and 0 in max_derivs:\n",
    "                to_pop = []\n",
    "                for i, deriv in enumerate(max_derivs):\n",
    "                    if deriv == 0:\n",
    "                        if [new_parents[i], child] not in bad_parents:\n",
    "                            bad_parents.append([new_parents[i].replace('ё', 'е'), child.replace('ё', 'е')])\n",
    "                        to_pop.append(i)\n",
    "                to_pop = to_pop[::-1]\n",
    "                for i in to_pop:\n",
    "                    same_roots.pop(i)\n",
    "                    newer_parents.pop(i)\n",
    "                    max_derivs.pop(i)\n",
    "                    begins.pop(i)\n",
    "                    ends.pop(i)\n",
    "            new_parents = copy(newer_parents)\n",
    "\n",
    "\n",
    "            #get rid of not beginnings\n",
    "            if 1 in begins and 0 in begins:\n",
    "                to_pop = []\n",
    "                for i, begin in enumerate(begins):\n",
    "                    if begin == 0:\n",
    "                        if [new_parents[i], child] not in bad_parents:\n",
    "                            bad_parents.append([new_parents[i].replace('ё', 'е'), child.replace('ё', 'е')])\n",
    "                        to_pop.append(i)\n",
    "                to_pop = to_pop[::-1]\n",
    "                for i in to_pop:\n",
    "                    same_roots.pop(i)\n",
    "                    newer_parents.pop(i)\n",
    "                    max_derivs.pop(i)\n",
    "                    begins.pop(i)\n",
    "                    ends.pop(i)\n",
    "            new_parents = copy(newer_parents)\n",
    "\n",
    "\n",
    "            #get rid of not endings\n",
    "            if 1 in ends and 0 in ends:\n",
    "                to_pop = []\n",
    "                for i, end in enumerate(ends):\n",
    "                    if end == 0:\n",
    "                        if [new_parents[i], child] not in bad_parents:\n",
    "                            bad_parents.append([new_parents[i].replace('ё', 'е'), child.replace('ё', 'е')])\n",
    "                        to_pop.append(i)\n",
    "                to_pop = to_pop[::-1]\n",
    "                for i in to_pop:\n",
    "                    same_roots.pop(i)\n",
    "                    newer_parents.pop(i)\n",
    "                    max_derivs.pop(i)\n",
    "                    begins.pop(i)\n",
    "                    ends.pop(i)\n",
    "            new_parents = copy(newer_parents)\n",
    "\n",
    "\n",
    "            clean_parents[child] = new_parents\n",
    "    parents = clean_parents\n",
    "    new_length = len(bad_parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_parents = {}\n",
    "for child in parents:\n",
    "    new_parents = copy(parents[child])\n",
    "    if child.endswith('ть') or child.endswith('ться'):\n",
    "        if child.startswith('пере') or child.startswith('про') or child.startswith('за') or child.startswith('на'):\n",
    "            for parent in parents[child]:\n",
    "                if parent.endswith('ть') or parent.endswith('ться'):\n",
    "                    if parent.startswith('пере') or parent.startswith('про') or parent.startswith('за') or parent.startswith('на'):\n",
    "                        if not set(kuzwords[parent][1]) & set(kuzwords[child][1]):\n",
    "                            new_parents.remove(parent)\n",
    "                            clean_parents[child] = new_parents\n",
    "                            if [parent, child] not in bad_parents:\n",
    "                                bad_parents.append([parent, child])\n",
    "parents = clean_parents\n",
    "new_length = len(bad_parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19532"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bad_parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create one dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one kuzwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = {}\n",
    "for w in kuzwords:\n",
    "    if w not in poses:\n",
    "        poses[w] = [kuzwords[w][2]]\n",
    "    else:\n",
    "        poses[w].append(kuzwords[w][2])\n",
    "for w in tix_poses:\n",
    "    if w not in poses:\n",
    "        poses[w] = [tix_poses[w]]\n",
    "    else:\n",
    "        poses[w].append(tix_poses[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98100"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one tixwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59563"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tixwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = {}\n",
    "for w in tixwords:\n",
    "    if w not in children:\n",
    "        children[w] = tixwords[w]\n",
    "    else:\n",
    "        children[w].extend(tixwords[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59563"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_all.iterrows():\n",
    "    parent = row['parent'].replace('ё', 'е')\n",
    "    child = row['child'].replace('ё', 'е')\n",
    "    if [parent, child] not in bad_parents:\n",
    "        if parent not in children and parent != child:\n",
    "            children[parent] = [child]\n",
    "        elif parent != child:\n",
    "            children[parent].append([child])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75995"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13864"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_parents = []\n",
    "all_children = []\n",
    "for parent in children:\n",
    "    all_children.extend(children[parent])\n",
    "for parent in children:\n",
    "    if parent not in all_children:\n",
    "        no_parents.append(parent)\n",
    "len(no_parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124014"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_children + no_parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create test bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parent_check.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "    i = 0\n",
    "    for child in random.sample(parents.keys(), 250):\n",
    "        if i < 100 and child not in tixwords:\n",
    "            i += 1\n",
    "            row = (child + ' ' + ' '.join(parents[child])).split(' ')\n",
    "            csvwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('no_parent_check.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "    i = 0\n",
    "    for child in random.sample(no_parents, 250):\n",
    "        if i < 100 and child not in tixwords:\n",
    "            i += 1\n",
    "            csvwriter.writerow([child])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_masks_tixwords = {}\n",
    "with open('nests.json', encoding='utf-8') as json_file:\n",
    "    nods = json.load(json_file)\n",
    "    for nod in nods:\n",
    "        for n in nod.values():\n",
    "            for word in n:\n",
    "                if 'знать' == cleantix(word['root']) and 'II' in word['root']:\n",
    "                    w = 'знать#N'\n",
    "                elif 'знать' == cleantix(word['root']):\n",
    "                    w = 'знать#V'\n",
    "                elif 'честь' == cleantix(word['root']) and 'II' in word['root']:\n",
    "                    w = 'честь#V'\n",
    "                elif 'честь' == cleantix(word['root']):\n",
    "                    w = 'честь#N'\n",
    "                elif 'печь' == cleantix(word['root']) and 'II' in word['root']:\n",
    "                    w = 'печь#V'\n",
    "                elif 'печь' == cleantix(word['root']):\n",
    "                    w = 'печь#N'\n",
    "                elif 'мочь' == cleantix(word['root']) and 'II' in word['root']:\n",
    "                    w = 'мочь#N'\n",
    "                elif 'мочь' == cleantix(word['root']):\n",
    "                    w = 'мочь#V'\n",
    "                elif cleantix(word['root']) in poses:\n",
    "                    w = cleantix(word['root']) + '#' + poses[cleantix(word['root'])][0]\n",
    "                if w not in word_masks_tixwords and cleantix(word['root']) in poses:\n",
    "                    derivates = []\n",
    "                    for deriv in word['ambiguous']:\n",
    "                        if cleantix(deriv) in poses:\n",
    "                            d = cleantix(deriv) + '#' + poses[cleantix(deriv)][0]\n",
    "                            if d not in derivates and d != w:\n",
    "                                derivates.append(d)\n",
    "                    for deriv in word['non-ambiguous']:\n",
    "                        if cleantix(deriv) in poses:\n",
    "                            d = cleantix(deriv) + '#' + poses[cleantix(deriv)][0]\n",
    "                            if d not in derivates and d != w:\n",
    "                                derivates.append(d)\n",
    "                    word_masks_tixwords[w] = derivates\n",
    "                elif cleantix(word['root']) in poses:\n",
    "                    derivates = word_masks_tixwords[w]\n",
    "                    for deriv in word['ambiguous']:\n",
    "                        if cleantix(deriv) in poses:\n",
    "                            d = cleantix(deriv) + '#' + poses[cleantix(deriv)][0]\n",
    "                            if d not in derivates and d != w:\n",
    "                                derivates.append(d)\n",
    "                    for deriv in word['non-ambiguous']:\n",
    "                        if cleantix(deriv) in poses:\n",
    "                            d = cleantix(deriv) + '#' + poses[cleantix(deriv)][0]\n",
    "                            if d not in derivates and d != w:\n",
    "                                derivates.append(d)\n",
    "                    word_masks_tixwords[w] = derivates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59518"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_masks_tixwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in word_masks_tixwords:\n",
    "    if '#' not in w:\n",
    "        print(w)\n",
    "    for w2 in word_masks_tixwords[w]:\n",
    "        if '#' not in w2:\n",
    "            print(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_all.iterrows():\n",
    "    parent = row['parent'].replace('ё', 'е')\n",
    "    child = row['child'].replace('ё', 'е')\n",
    "    parent = parent + '#' + poses[parent][0]\n",
    "    child = child + '#' + poses[child][0]\n",
    "    if [parent, child] not in bad_parents:\n",
    "        if parent not in word_masks_tixwords and parent != child:\n",
    "            word_masks_tixwords[parent] = [child]\n",
    "        elif parent != child:\n",
    "            word_masks_tixwords[parent].append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79408"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_masks_tixwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5656"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_parents = []\n",
    "word_masks_children = []\n",
    "for parent in word_masks_tixwords:\n",
    "    word_masks_children.extend(word_masks_tixwords[parent])\n",
    "for parent in word_masks_tixwords:\n",
    "    if parent not in word_masks_children:\n",
    "        no_parents.append(parent)\n",
    "len(no_parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get two rooted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_parents = {}\n",
    "for parent in word_masks_tixwords:\n",
    "    for child in word_masks_tixwords[parent]:\n",
    "        new_child = child\n",
    "        if type(child) == list:\n",
    "            new_child = new_child[0]\n",
    "            \n",
    "            \n",
    "        if new_child not in morph_parents:\n",
    "            morph_parents[new_child] = [parent]\n",
    "        else:\n",
    "            morph_parents[new_child].append(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "global two_parents \n",
    "two_parents = {}\n",
    "for child in morph_parents:\n",
    "    new_child = child\n",
    "    if type(child) == list:\n",
    "            new_child = new_child[0]\n",
    "    raw_child = child.split('#')[0]\n",
    "            \n",
    "            \n",
    "    if len(morph_parents[new_child]) == 2:\n",
    "        parent1 = morph_parents[new_child][0].split('#')[0]\n",
    "        parent2 = morph_parents[new_child][1].split('#')[0]\n",
    "        if parent1 in kuzwords and parent2 in kuzwords:\n",
    "            if len(set(kuzwords[parent1][1]) & set(kuzwords[parent2][1])) == 0:\n",
    "                two_parents[new_child] = [morph_parents[new_child][0], morph_parents[new_child][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "global loops\n",
    "loops = {}\n",
    "for child in parents:\n",
    "    for parent in parents[child]:\n",
    "        if parent in parents:\n",
    "            if child in parents[parent]:\n",
    "                if child not in loops:\n",
    "                    loops[child] = [parent]\n",
    "                else:\n",
    "                    loops[child].append(parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get to derinet format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 id 2 word#N 3 word 4 NOUN / ADJ / VERB / ADV 5 empty 6 empty 7 id 8 Type=Derivation Type=Com 9 empty 10 morphs + second parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_masks_tixwords['ворон#N'] = ['вороненый#A', 'вороненький#A', 'воронить#V','ворониха#N', 'вороновые#A']\n",
    "word_masks_tixwords['верша#N'] = []\n",
    "word_masks_tixwords['плат#N'] = ['платать#V', 'плахта#N']\n",
    "word_masks_tixwords['платать#V'] = []\n",
    "word_masks_tixwords['колок#N'] = ['коловорот#N', 'коловращение#N']\n",
    "word_masks_tixwords['колос#N'] = ['колосовые#A', 'колосистый#A', 'колосник#N', 'колосовой#A', 'пустоколосый#A', 'колосок#N', 'колоситься#V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "global current_max_width\n",
    "current_max_width = 0\n",
    "\n",
    "global trees_max_width\n",
    "trees_max_width = []\n",
    "\n",
    "global trees_max_depth\n",
    "trees_max_depth = []\n",
    "\n",
    "global current_max_depth\n",
    "current_max_depth = 0\n",
    "\n",
    "global current_depth\n",
    "current_depth = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_bad_verb_parent(parent, child, kuzwords):\n",
    "    true = True\n",
    "    suffixes = ['вы','пере','про', 'при', 'у', 'за','из', 'на', 'по', 'под', 'с', 'о', 'рас', 'раз', 'в', 'до']\n",
    "    if parent.endswith('ть') or parent.endswith('ться'):\n",
    "        for s in suffixes:\n",
    "            if parent.startswith(s):\n",
    "                if child in kuzwords and parent in kuzwords:\n",
    "                    if not set(kuzwords[parent][1]) & set(kuzwords[child][1]):\n",
    "                        true = False\n",
    "    if child.endswith('ть') or child.endswith('ться'):\n",
    "        for s in suffixes:\n",
    "            if child.startswith(s):\n",
    "                if child in kuzwords and parent in kuzwords:\n",
    "                    if not set(kuzwords[parent][1]) & set(kuzwords[child][1]):\n",
    "                        true = False\n",
    "    other = ['не','само']\n",
    "    for o in other:\n",
    "        if parent.startswith(o) or child.startswith(o):\n",
    "            if parent in kuzwords and child in kuzwords:\n",
    "                if not set(kuzwords[parent][1]) & set(kuzwords[child][1]):\n",
    "                    true = False\n",
    "            else:\n",
    "                if parent not in child and child not in parent:\n",
    "                    true = False\n",
    "    return true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_derinet(t, parent_id, parent, child_tag):\n",
    "    global current_max_width\n",
    "    global trees_max_width\n",
    "    global trees_max_depth\n",
    "    global current_max_depth\n",
    "    global current_depth\n",
    "    \n",
    "    if type(child_tag) == list:\n",
    "        child_tag = child_tag[0]\n",
    "        \n",
    "    \n",
    "    if t > current_max_width:\n",
    "        current_max_width = t\n",
    "    if t > trees_max_width[-1]:\n",
    "        trees_max_width[-1] = t\n",
    "        \n",
    "\n",
    "    current_depth += 1\n",
    "    if current_depth > current_max_depth:\n",
    "        current_max_depth = current_depth\n",
    "    if trees_max_depth[-1] < current_max_depth:\n",
    "        trees_max_depth[-1] = current_max_depth\n",
    "\n",
    "        \n",
    "    child_id = parent_id.split('.')[0] + '.' + str(t) \n",
    "    t += 1\n",
    "    ids.append(child_id)\n",
    "    \n",
    "    \n",
    "    mask = child_tag\n",
    "    masks.append(child_tag)   \n",
    "    \n",
    "    \n",
    "    word = child_tag.split('#')[0]\n",
    "    words.append(word)\n",
    "    \n",
    "    \n",
    "    pos = child_tag.split('#')[-1]\n",
    "    if pos == 'N':\n",
    "        full_pos = 'NOUN'\n",
    "    elif pos == 'A':\n",
    "        full_pos = 'ADJ'\n",
    "    elif pos == 'V':\n",
    "        full_pos = 'VERB'\n",
    "    elif pos == 'D':\n",
    "        full_pos = 'ADV'\n",
    "    full_poses.append(full_pos)\n",
    "    \n",
    "    \n",
    "    if word in kuzwords:\n",
    "        morph = 'morphs: ' + ', '.join(kuzwords[word][0]) + '; roots: '+ ', '.join(kuzwords[word][1])\n",
    "    else:\n",
    "        morph = ''\n",
    "    morphs.append(morph)\n",
    "    \n",
    "    \n",
    "    par_id = parent_id\n",
    "    parent_ids.append(par_id)\n",
    "    \n",
    "    \n",
    "    if mask in two_parents:\n",
    "        derivation = 'Type=Com'\n",
    "        second_parent = 'parents: ' + ', '.join(two_parents[mask])\n",
    "    else:\n",
    "        derivation = 'Type=Derivation'\n",
    "        second_parent = ''\n",
    "    derivations.append(derivation)\n",
    "    second_parents.append(second_parent)\n",
    "    \n",
    "    \n",
    "    if child_tag in word_masks_tixwords and word not in loops and child_tag not in used:\n",
    "        for child in sorted(word_masks_tixwords[child_tag]):\n",
    "            if type(child) == list:\n",
    "                new_child = child[0]\n",
    "            else:\n",
    "                new_child = child\n",
    "            if is_not_bad_verb_parent(word, new_child.split('#')[0], kuzwords):\n",
    "                used.append(child_tag)\n",
    "                t = to_derinet(t, child_id, word, new_child)\n",
    "                current_depth -= 1\n",
    "    \n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "global ids\n",
    "ids = []\n",
    "global masks\n",
    "masks = []\n",
    "global words\n",
    "words = []\n",
    "global full_poses\n",
    "full_poses = []\n",
    "global morphs\n",
    "morphs = []\n",
    "global parent_ids\n",
    "parent_ids = []\n",
    "global derivations\n",
    "derivations = []\n",
    "global second_parents\n",
    "second_parents = []\n",
    "global used\n",
    "used = []\n",
    "global t\n",
    "t = 1\n",
    "\n",
    "\n",
    "for i, parent in enumerate(sorted(no_parents)):\n",
    "    \n",
    "    \n",
    "    child_id = str(i+1) + '.0'\n",
    "    t = 1\n",
    "    ids.append(child_id)\n",
    "    \n",
    "    \n",
    "    mask = parent\n",
    "    masks.append(mask)\n",
    "    \n",
    "    \n",
    "    word = parent.split('#')[0]\n",
    "    words.append(word)\n",
    "    \n",
    "    \n",
    "    pos = parent.split('#')[-1]\n",
    "    if pos == 'N':\n",
    "        full_pos = 'NOUN'\n",
    "    elif pos == 'A':\n",
    "        full_pos = 'ADJ'\n",
    "    elif pos == 'V':\n",
    "        full_pos = 'VERB'\n",
    "    elif pos == 'D':\n",
    "        full_pos = 'ADV'\n",
    "    full_poses.append(full_pos)\n",
    "    \n",
    "    \n",
    "    if word in kuzwords:\n",
    "        morph = 'morphs: ' + ', '.join(kuzwords[word][0]) + '; roots: '+ ', '.join(kuzwords[word][1])\n",
    "    else:\n",
    "        morph = ''\n",
    "    morphs.append(morph)\n",
    "    \n",
    "    \n",
    "    parent_id = ''\n",
    "    parent_ids.append(parent_id)\n",
    "    \n",
    "    \n",
    "    derivation = ''\n",
    "    derivations.append(derivation)\n",
    "    \n",
    "    \n",
    "    second_parent = ''\n",
    "    second_parents.append(second_parent)\n",
    "    \n",
    "    if parent in word_masks_tixwords:\n",
    "        for child in sorted(word_masks_tixwords[parent]):\n",
    "            current_depth = 0\n",
    "            current_max_depth = 0\n",
    "            trees_max_depth.append(0)\n",
    "            current_max_width = 0\n",
    "            trees_max_width.append(0)\n",
    "            \n",
    "            if type(child) == list:\n",
    "                new_child = child[0]\n",
    "            else:\n",
    "                new_child = child\n",
    "            \n",
    "            if is_not_bad_verb_parent(word, new_child.split('#')[0], kuzwords):\n",
    "                t = to_derinet(t, child_id, word, new_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "649"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(trees_max_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(trees_max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.08394327927316"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(trees_max_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4547168209221364"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(trees_max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "with open('ru-der.tsv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "    \n",
    "    \n",
    "    for i in range(len(ids)):\n",
    "        child_id = ids[i]\n",
    "        mask = masks[i]\n",
    "        word = words[i]\n",
    "        full_pos = full_poses[i]\n",
    "        empty = ''\n",
    "        morph = morphs[i]\n",
    "        parent_id = parent_ids[i]\n",
    "        derivation = derivations[i]\n",
    "        second_parent = second_parents[i]\n",
    "        \n",
    "        \n",
    "        if int(child_id.split('.')[0]) != t:\n",
    "            t = int(child_id.split('.')[0])\n",
    "            csvwriter.writerow(['', '', '', '', '', '', '', '', '', ''])\n",
    "            \n",
    "        csvwriter.writerow([child_id, mask, word, full_pos, empty, empty, parent_id, derivation, empty, ' '.join([morph, second_parent])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "duplicates = []\n",
    "with open('ru-der-no-duplicates.tsv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "    \n",
    "    \n",
    "    for i in range(len(ids)):\n",
    "        child_id = ids[i]\n",
    "        mask = masks[i]\n",
    "        word = words[i]\n",
    "        full_pos = full_poses[i]\n",
    "        empty = ''\n",
    "        morph = morphs[i]\n",
    "        parent_id = parent_ids[i]\n",
    "        derivation = derivations[i]\n",
    "        second_parent = second_parents[i]\n",
    "        \n",
    "        \n",
    "        if mask not in duplicates:\n",
    "            duplicates.append(mask)\n",
    "            if int(child_id.split('.')[0]) != t:\n",
    "                t = int(child_id.split('.')[0])\n",
    "                csvwriter.writerow(['', '', '', '', '', '', '', '', '', ''])\n",
    "\n",
    "\n",
    "            csvwriter.writerow([child_id, mask, word, full_pos, empty, empty, parent_id, derivation, empty, ' '.join([morph, second_parent])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
